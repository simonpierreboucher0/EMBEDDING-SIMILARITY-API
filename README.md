# ğŸŒ Embedding API Gateway ğŸš€

## ğŸ”¥ Passerelle universelle pour tous vos services d'embeddings et recherche sÃ©mantique ğŸ”¥

[![GitHub stars](https://img.shields.io/github/stars/simonpierreboucher0/embedding-api?style=social)](https://github.com/simonpierreboucher0/embedding-api/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.105.0+-green.svg)](https://fastapi.tiangolo.com/)

---

## âœ¨ CaractÃ©ristiques principales

ğŸ”„ **Interface unifiÃ©e** - Une API pour tous les fournisseurs d'embeddings  
ğŸ§© **Double moteur de recherche** - FAISS ou SimilaritÃ© Cosinus au choix  
ğŸ” **Recherche vectorielle** - Recherche sÃ©mantique ultra-rapide avec FAISS  
ğŸ“Š **Stockage flexible** - Formats optimisÃ©s pour rapiditÃ© ou lisibilitÃ©  
âš¡ **Hautes performances** - Optimisations pour grands volumes de donnÃ©es  
ğŸ§  **Gestion d'index** - CrÃ©ation, mise Ã  jour et suppression simplifiÃ©es  
ğŸ”’ **SÃ©curitÃ© intÃ©grÃ©e** - Gestion sÃ©curisÃ©e des clÃ©s API  
ğŸŒˆ **Comparaison directe** - Calcul instantanÃ© de similaritÃ© entre textes  

---

## ğŸ¤– Fournisseurs et modÃ¨les pris en charge

### ğŸŸ¢ OpenAI
ModÃ¨les d'embeddings leaders du marchÃ©.

| ModÃ¨le | Dimension | Cas d'utilisation |
|--------|-----------|-------------------|
| ğŸŒŸ **text-embedding-ada-002** | 1536 | Recherche sÃ©mantique, classification, clustering |
| ğŸ’ **text-embedding-3-small** | 1536 | Applications avec contraintes de coÃ»t, performance Ã©quilibrÃ©e |
| ğŸ† **text-embedding-3-large** | 3072 | Recherche haute prÃ©cision, applications premium |

### ğŸŸ£ Cohere
ModÃ¨les d'embeddings spÃ©cialisÃ©s et multilingues.

| ModÃ¨le | Cas d'utilisation |
|--------|-------------------|
| ğŸŒ  **embed-english-v3.0** | Recherche sÃ©mantique optimisÃ©e pour l'anglais |
| ğŸŒ **embed-multilingual-v3.0** | Applications multilingues, recherche cross-language |
| âš¡ **embed-english-light-v3.0** | Version lÃ©gÃ¨re et rapide pour l'anglais |
| ğŸš€ **embed-multilingual-light-v3.0** | Version lÃ©gÃ¨re multilingue, faible latence |

### ğŸ”µ Mistral AI
ModÃ¨le d'embedding puissant conÃ§u en France.

| ModÃ¨le | Cas d'utilisation |
|--------|-------------------|
| ğŸ’« **mistral-embed** | Recherche sÃ©mantique, RAG, classification de documents |

### ğŸŸ¡ DeepInfra
Large sÃ©lection de modÃ¨les d'embeddings open-source optimisÃ©s.

| ModÃ¨le | Cas d'utilisation |
|--------|-------------------|
| ğŸ” **BAAI/bge-base-en-v1.5** | ModÃ¨le Ã©quilibrÃ© pour l'anglais |
| ğŸ” **BAAI/bge-small-en-v1.5** | Version compacte, faible empreinte mÃ©moire |
| ğŸ”¬ **BAAI/bge-large-en-v1.5** | PrÃ©cision Ã©levÃ©e pour applications critiques |
| ğŸŒŸ **thenlper/gte-large** | ModÃ¨le gÃ©nÃ©ral avec large couverture sÃ©mantique |
| â­ **thenlper/gte-base** | Version Ã©quilibrÃ©e pour usage gÃ©nÃ©ral |
| ğŸ“Š **intfloat/e5-large-v2** | OptimisÃ© pour RAG et recherche avancÃ©e |
| ğŸ§® **sentence-transformers/all-mpnet-base-v2** | Classique pour transformation de phrases |

### ğŸŸ  Voyage AI
ModÃ¨les d'embeddings spÃ©cialisÃ©s avec capacitÃ©s avancÃ©es.

| ModÃ¨le | Cas d'utilisation |
|--------|-------------------|
| ğŸŒŠ **voyage-large-2** | Recherche sÃ©mantique de pointe, applications premium |
| ğŸ§  **voyage-large-2-instruct** | Recherche sÃ©mantique guidÃ©e par instructions |
| ğŸ’» **voyage-code-2** | SpÃ©cialisÃ© pour code source et documentation technique |

---

## ğŸ› ï¸ Installation facile en 3 Ã©tapes

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/simonpierreboucher0/embedding-api.git
cd embedding-api
```

### 2ï¸âƒ£ Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configurer vos clÃ©s API
CrÃ©ez un fichier `.env` avec vos clÃ©s:
```ini
OPENAI_API_KEY=sk-xxxx
COHERE_API_KEY=xxxx
MISTRAL_API_KEY=xxxx
DEEPINFRA_API_KEY=xxxx
VOYAGE_API_KEY=xxxx
```

> ğŸ’¡ **Astuce**: Vous n'avez besoin de fournir que les clÃ©s pour les fournisseurs que vous allez utiliser!

---

## ğŸš€ Guide d'utilisation rapide

### â–¶ï¸ DÃ©marrer le serveur
```bash
uvicorn app:app --reload --host 0.0.0.0 --port 8001
```

### ğŸ“‹ Obtenir la liste des modÃ¨les
```bash
curl -X GET http://localhost:8001/models
```

### ğŸ§  GÃ©nÃ©rer des embeddings
```bash
curl -X POST http://localhost:8001/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-ada-002",
    "texts": "Voici un exemple de texte pour gÃ©nÃ©rer un embedding."
  }'
```

### ğŸ—„ï¸ CrÃ©er un index FAISS
```bash
curl -X POST http://localhost:8001/index \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-ada-002",
    "index_name": "mon_index_articles",
    "texts": [
      "L'intelligence artificielle rÃ©volutionne notre monde.",
      "Le machine learning permet d'automatiser des tÃ¢ches complexes.",
      "La recherche sÃ©mantique utilise des vecteurs d'embedding.",
      "FAISS est une bibliothÃ¨que efficace pour la recherche vectorielle.",
      "Les LLMs utilisent des transformers pour comprendre le contexte."
    ],
    "method": "faiss"
  }'
```

### ğŸ“Š CrÃ©er un index avec similaritÃ© cosinus
```bash
curl -X POST http://localhost:8001/index \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-ada-002",
    "index_name": "mon_index_json",
    "texts": [
      "L'intelligence artificielle rÃ©volutionne notre monde.",
      "Le machine learning permet d'automatiser des tÃ¢ches complexes.",
      "La recherche sÃ©mantique utilise des vecteurs d'embedding.",
      "FAISS est une bibliothÃ¨que efficace pour la recherche vectorielle.",
      "Les LLMs utilisent des transformers pour comprendre le contexte."
    ],
    "method": "cosine"
  }'
```

### ğŸ” Recherche sÃ©mantique
```bash
curl -X POST http://localhost:8001/search \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-ada-002",
    "index_name": "mon_index_articles",
    "query": "Comment fonctionne l'IA?",
    "top_k": 3,
    "method": "faiss"
  }'
```

### ğŸ”„ Mettre Ã  jour un index existant
```bash
curl -X PUT http://localhost:8001/index/mon_index_articles \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-ada-002",
    "texts": [
      "Les rÃ©seaux de neurones sont inspirÃ©s du cerveau humain.",
      "Le deep learning est une sous-catÃ©gorie du machine learning.",
      "Les embeddings permettent de capturer la sÃ©mantique du texte."
    ],
    "method": "faiss"
  }'
```

### ğŸ§® Comparer deux textes directement
```bash
curl -X POST "http://localhost:8001/compare?texts=L'IA%20est%20revolutionnaire&texts=L'intelligence%20artificielle%20transforme%20le%20monde&provider=openai&model=text-embedding-ada-002" \
  -H "accept: application/json"
```

---

## ğŸ“Š Structure complÃ¨te des requÃªtes et rÃ©ponses

### ğŸ“ GÃ©nÃ©ration d'embeddings

**RequÃªte:**
```json
{
  "provider": "openai",              // ğŸŒ Fournisseur (obligatoire)
  "model": "text-embedding-ada-002", // ğŸ¤– ModÃ¨le spÃ©cifique (obligatoire)
  "texts": ["Premier texte", "DeuxiÃ¨me texte"], // ğŸ“„ Textes (string ou array)
  "encoding_format": "float",        // ğŸ“Š Format d'encodage (pour certains providers)
  "input_type": "classification"     // ğŸ·ï¸ Type d'entrÃ©e (pour Cohere)
}
```

**RÃ©ponse:**
```json
{
  "embeddings": [                    // ğŸ§  Liste des vecteurs d'embedding
    [0.0023, -0.0118, 0.0094, ...],  // ğŸ“Š Premier vecteur
    [0.0089, -0.0342, 0.0211, ...]   // ğŸ“Š DeuxiÃ¨me vecteur
  ],
  "model": "text-embedding-ada-002", // ğŸ¤– ModÃ¨le utilisÃ©
  "provider": "openai",              // ğŸŒ Fournisseur utilisÃ©
  "dimension": 1536,                 // ğŸ“ Dimension des vecteurs
  "total_tokens": 14                 // ğŸ”¢ Nombre de tokens (si disponible)
}
```

### ğŸ—„ï¸ CrÃ©ation d'index

**RequÃªte:**
```json
{
  "provider": "openai",              // ğŸŒ Fournisseur (obligatoire)
  "model": "text-embedding-ada-002", // ğŸ¤– ModÃ¨le spÃ©cifique (obligatoire)
  "index_name": "mon_index",         // ğŸ“š Nom de l'index (obligatoire)
  "texts": [                         // ğŸ“„ Textes Ã  indexer (obligatoire)
    "Premier document Ã  indexer",
    "DeuxiÃ¨me document Ã  indexer",
    "TroisiÃ¨me document Ã  indexer"
  ],
  "method": "faiss",                 // ğŸ” MÃ©thode d'indexation: "faiss" ou "cosine"
  "encoding_format": "float",        // ğŸ“Š Format d'encodage (optionnel)
  "input_type": "classification"     // ğŸ·ï¸ Type d'entrÃ©e (pour Cohere, optionnel)
}
```

**RÃ©ponse:**
```json
{
  "provider": "openai",              // ğŸŒ Fournisseur utilisÃ©
  "model": "text-embedding-ada-002", // ğŸ¤– ModÃ¨le utilisÃ©
  "index_name": "mon_index",         // ğŸ“š Nom de l'index crÃ©Ã©
  "total_chunks": 3,                 // ğŸ§© Nombre de documents indexÃ©s
  "created_at": "2023-08-15T14:23:45.123456", // â° Date de crÃ©ation
  "dimension": 1536,                 // ğŸ“ Dimension des vecteurs
  "method": "faiss"                  // ğŸ” MÃ©thode d'indexation utilisÃ©e
}
```

### ğŸ” Recherche sÃ©mantique

**RequÃªte:**
```json
{
  "provider": "openai",              // ğŸŒ Fournisseur (obligatoire)
  "model": "text-embedding-ada-002", // ğŸ¤– ModÃ¨le spÃ©cifique (obligatoire)
  "index_name": "mon_index",         // ğŸ“š Nom de l'index (obligatoire)
  "query": "Ma requÃªte de recherche", // ğŸ” Texte de la requÃªte (obligatoire)
  "top_k": 5,                        // ğŸ” Nombre de rÃ©sultats souhaitÃ©s
  "method": "faiss"                  // ğŸ” MÃ©thode de recherche: "faiss" ou "cosine"
}
```

**RÃ©ponse:**
```json
{
  "index_name": "mon_index",         // ğŸ“š Nom de l'index utilisÃ©
  "provider": "openai",              // ğŸŒ Fournisseur utilisÃ©
  "model": "text-embedding-ada-002", // ğŸ¤– ModÃ¨le utilisÃ©
  "query": "Ma requÃªte de recherche", // ğŸ” Texte de la requÃªte
  "method": "faiss",                 // ğŸ” MÃ©thode utilisÃ©e
  "results": [                       // ğŸ“‹ RÃ©sultats de recherche
    {
      "chunk_id": 2,                 // ğŸ†” ID du document
      "text": "DeuxiÃ¨me document Ã  indexer", // ğŸ“„ Texte du document
      "distance": 0.125,             // ğŸ“ Distance (FAISS uniquement)
      "similarity": 0.89,            // ğŸ“Š Score de similaritÃ©
      "rank": 1                      // ğŸ… Rang dans les rÃ©sultats
    },
    // ... autres rÃ©sultats
  ]
}
```

---

## ğŸ§ª Comparaison des mÃ©thodes de recherche

### ğŸš€ FAISS (Fast Library for Approximate Nearest Neighbors)

**Avantages:**
- âš¡ **Ultra rapide** pour les grands ensembles de donnÃ©es
- ğŸ” **Recherche approximative** optimisÃ©e pour hautes dimensions
- ğŸ“ˆ **Passage Ã  l'Ã©chelle** efficace avec des millions de vecteurs
- ğŸ§  **Optimisations mÃ©moire** pour les grandes collections

**Utilisez FAISS quand:**
- ğŸ—„ï¸ Vous avez de grandes collections de documents (>10K textes)
- â±ï¸ La vitesse de recherche est critique
- ğŸ’½ Vous avez des contraintes de mÃ©moire pour de grands index

### ğŸ“Š SimilaritÃ© Cosinus (avec stockage JSON)

**Avantages:**
- ğŸ“‹ **Simple et transparent** - stockage en fichiers JSON lisibles
- ğŸ¯ **PrÃ©cision Ã©levÃ©e** pour les petites collections
- ğŸ”§ **Facile Ã  dÃ©boguer** et Ã  inspecter manuellement
- ğŸ”„ **Portable** entre diffÃ©rents environnements

**Utilisez Cosinus quand:**
- ğŸ“ Vous avez de petites collections (< 10K textes)
- ğŸ‘ï¸ La lisibilitÃ©/transparence des donnÃ©es est importante
- ğŸ§ª Vous prototypez ou testez votre application

---

## ğŸ’¡ Cas d'utilisation avancÃ©s

### ğŸ”„ ChaÃ®ne de traitement RAG (Retrieval Augmented Generation)

```python
import requests
import json

# 1. Indexer des documents techniques
documents = [
    "FastAPI est un framework web moderne pour Python.",
    "FAISS est une bibliothÃ¨que pour la recherche de similaritÃ© efficace.",
    "Les embeddings vectoriels reprÃ©sentent le texte dans un espace multidimensionnel.",
    "La recherche sÃ©mantique permet de trouver des informations par leur signification.",
    "La distance cosinus est une mesure courante de similaritÃ© entre vecteurs."
]

# CrÃ©er l'index
requests.post(
    "http://localhost:8001/index",
    json={
        "provider": "openai",
        "model": "text-embedding-ada-002",
        "index_name": "docs_techniques",
        "texts": documents,
        "method": "faiss"
    }
)

# 2. Quand l'utilisateur pose une question
user_query = "Comment fonctionne la recherche vectorielle?"

# 3. Rechercher les documents pertinents
search_response = requests.post(
    "http://localhost:8001/search",
    json={
        "provider": "openai",
        "model": "text-embedding-ada-002",
        "index_name": "docs_techniques",
        "query": user_query,
        "top_k": 2,
        "method": "faiss"
    }
)

relevant_docs = [result["text"] for result in search_response.json()["results"]]

# 4. Construire un prompt enrichi pour un LLM
context = "\n".join(relevant_docs)
prompt = f"""
Contexte:
{context}

Question: {user_query}

RÃ©ponse basÃ©e sur le contexte donnÃ©:
"""

# 5. Envoyer Ã  un LLM (via un service externe comme OpenAI)
# Code pour appeler un LLM avec le prompt enrichi
print("Prompt enrichi avec contexte:", prompt)
```

### ğŸ§ª Test A/B de diffÃ©rents modÃ¨les d'embeddings

```python
import requests
import numpy as np
from sklearn.metrics import precision_recall_fscore_support

# Liste de modÃ¨les Ã  comparer
models_to_test = [
    {"provider": "openai", "model": "text-embedding-ada-002"},
    {"provider": "openai", "model": "text-embedding-3-small"},
    {"provider": "cohere", "model": "embed-english-v3.0"},
    {"provider": "mistral", "model": "mistral-embed"}
]

# Ensemble de test (requÃªtes et documents pertinents attendus)
test_queries = [
    {
        "query": "Comment fonctionne l'apprentissage profond?",
        "relevant_docs": [0, 2, 5]  # Indices des documents pertinents
    },
    {
        "query": "Qu'est-ce que le traitement du langage naturel?",
        "relevant_docs": [1, 4, 7]
    }
    # Ajoutez plus de requÃªtes...
]

documents = [
    "Le deep learning est un sous-domaine du machine learning qui utilise des rÃ©seaux de neurones.",
    "Le NLP ou traitement du langage naturel permet aux machines de comprendre le texte.",
    "Les rÃ©seaux de neurones profonds contiennent plusieurs couches cachÃ©es.",
    "Python est un langage de programmation populaire pour l'IA.",
    "BERT est un modÃ¨le de langage prÃ©-entraÃ®nÃ© pour le NLP.",
    "L'apprentissage profond nÃ©cessite gÃ©nÃ©ralement de grandes quantitÃ©s de donnÃ©es.",
    "TensorFlow et PyTorch sont des frameworks populaires pour le deep learning.",
    "Les modÃ¨les de langue comme GPT utilisent le NLP pour gÃ©nÃ©rer du texte.",
    "Les embeddings vectoriels sont essentiels pour la recherche sÃ©mantique."
]

results = {}

# Tester chaque modÃ¨le
for model_info in models_to_test:
    provider = model_info["provider"]
    model = model_info["model"]
    
    # CrÃ©er un index avec ce modÃ¨le
    index_name = f"test_{provider}_{model.replace('-', '_').replace('/', '_')}"
    
    requests.post(
        "http://localhost:8001/index",
        json={
            "provider": provider,
            "model": model,
            "index_name": index_name,
            "texts": documents,
            "method": "faiss"
        }
    )
    
    # Ã‰valuer sur chaque requÃªte
    precision_scores = []
    recall_scores = []
    f1_scores = []
    
    for test_case in test_queries:
        query = test_case["query"]
        relevant_docs = test_case["relevant_docs"]
        
        # Faire la recherche
        search_response = requests.post(
            "http://localhost:8001/search",
            json={
                "provider": provider,
                "model": model,
                "index_name": index_name,
                "query": query,
                "top_k": len(documents),
                "method": "faiss"
            }
        )
        
        # RÃ©cupÃ©rer les rÃ©sultats
        results_data = search_response.json()["results"]
        retrieved_indices = [result["chunk_id"] for result in results_data]
        
        # CrÃ©er un vecteur de pertinence binaire pour l'Ã©valuation
        y_true = np.zeros(len(documents))
        y_true[relevant_docs] = 1
        
        y_pred = np.zeros(len(documents))
        y_pred[retrieved_indices[:5]] = 1  # ConsidÃ©rer les 5 premiers comme pertinents
        
        # Calculer les mÃ©triques
        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
        
        precision_scores.append(precision)
        recall_scores.append(recall)
        f1_scores.append(f1)
    
    # Stocker les rÃ©sultats pour ce modÃ¨le
    results[f"{provider}/{model}"] = {
        "precision_avg": np.mean(precision_scores),
        "recall_avg": np.mean(recall_scores),
        "f1_avg": np.mean(f1_scores)
    }
    
    # Nettoyer l'index aprÃ¨s le test
    requests.delete(f"http://localhost:8001/index/{index_name}?method=faiss")

# Afficher les rÃ©sultats comparatifs
print("\n=== RÃ‰SULTATS DE LA COMPARAISON DES MODÃˆLES ===")
for model_name, metrics in results.items():
    print(f"ğŸ“Š {model_name}:")
    print(f"  PrÃ©cision: {metrics['precision_avg']:.4f}")
    print(f"  Rappel: {metrics['recall_avg']:.4f}")
    print(f"  F1-Score: {metrics['f1_avg']:.4f}")
    print("---")
```

---

## ğŸ“š Guide des fonctionnalitÃ©s avancÃ©es

### ğŸ§® Comparaison directe de textes

Calculez rapidement la similaritÃ© entre deux textes sans crÃ©er d'index:

```bash
curl -X POST "http://localhost:8001/compare?texts=L'IA%20est%20revolutionnaire&texts=L'intelligence%20artificielle%20transforme%20le%20monde&provider=openai&model=text-embedding-ada-002"
```

RÃ©ponse:
```json
{
  "text1": "L'IA est revolutionnaire",
  "text2": "L'intelligence artificielle transforme le monde",
  "similarity": 0.8712,
  "provider": "openai",
  "model": "text-embedding-ada-002"
}
```

### ğŸ“‹ Listing des index disponibles

```bash
curl -X GET http://localhost:8001/indexes
```

RÃ©ponse:
```json
[
  {
    "provider": "openai",
    "model": "text-embedding-ada-002",
    "index_name": "mon_index_articles",
    "total_chunks": 8,
    "created_at": "2023-08-15T14:23:45.123456",
    "updated_at": "2023-08-16T09:12:34.567890",
    "dimension": 1536,
    "method": "faiss"
  },
  {
    "provider": "cohere",
    "model": "embed-english-v3.0",
    "index_name": "mon_index_json",
    "total_chunks": 5,
    "created_at": "2023-08-15T15:45:12.345678",
    "dimension": 1024,
    "method": "cosine"
  }
]
```

### ğŸ” Obtenir les informations sur un index spÃ©cifique

```bash
curl -X GET "http://localhost:8001/index/mon_index_articles?method=faiss"
```

RÃ©ponse:
```json
{
  "provider": "openai",
  "model": "text-embedding-ada-002",
  "index_name": "mon_index_articles",
  "total_chunks": 8,
  "created_at": "2023-08-15T14:23:45.123456",
  "updated_at": "2023-08-16T09:12:34.567890",
  "dimension": 1536,
  "method": "faiss"
}
```

---

## ğŸ”’ SÃ©curitÃ© et bonnes pratiques

- ğŸ”‘ **Gestion des clÃ©s**: StockÃ©es localement dans `.env`, jamais exposÃ©es
- ğŸ›¡ï¸ **CORS**: Configuration sÃ©curisÃ©e pour les requÃªtes cross-origin
- ğŸ“ **Logging**: DÃ©tails utiles sans informations sensibles
- ğŸ”„ **Rate Limiting**: Respect des limites de taux des fournisseurs
- ğŸ’¾ **Stockage local**: Les donnÃ©es restent sur votre serveur, pas d'envoi externe
- ğŸ§¹ **Nettoyage facile**: Suppression d'index via API simple

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues! Voici comment participer:

1. ğŸ´ **Fork** le dÃ©pÃ´t
2. ğŸ”„ CrÃ©ez une **branche** (`git checkout -b feature/ma-fonctionnalite`)
3. âœï¸ Faites vos **modifications**
4. ğŸ“¦ **Commit** vos changements (`git commit -m 'Ajout de ma fonctionnalitÃ©'`)
5. ğŸ“¤ **Push** vers la branche (`git push origin feature/ma-fonctionnalite`)
6. ğŸ” Ouvrez une **Pull Request**

### ğŸ’¼ IdÃ©es de contributions

- ğŸ§ª Support de nouveaux fournisseurs d'embeddings
- ğŸ“ AmÃ©lioration de la documentation
- âœ¨ FonctionnalitÃ©s avancÃ©es (clustering, chunking automatique)
- ğŸš€ Optimisations de performance
- ğŸŒ Support de la persistance dans des bases de donnÃ©es vectorielles

---

## ğŸ“œ Licence

Ce projet est sous licence [MIT](LICENSE) - voir le fichier LICENSE pour plus de dÃ©tails.

---

## â“ FAQ

### ğŸ”„ Quelle mÃ©thode de recherche choisir?
FAISS pour grands datasets et performance, Cosinus pour petits datasets et transparence.

### ğŸ” Quelle est la meilleure dimension pour les embeddings?
En gÃ©nÃ©ral, plus la dimension est Ã©levÃ©e, plus la prÃ©cision est grande, mais au prix de plus de ressources. 1536 est un bon compromis.

### ğŸ§© Comment chunker mes documents avant de les indexer?
Utilisez une bibliothÃ¨que comme LangChain ou LlamaIndex pour dÃ©couper vos documents avant de les envoyer Ã  l'API.

### ğŸ’° Comment optimiser les coÃ»ts des API d'embeddings?
GÃ©nÃ©rez les embeddings une seule fois et stockez-les. Utilisez des modÃ¨les plus lÃ©gers pour les cas d'usage moins critiques.

### ğŸ”§ Comment puis-je amÃ©liorer la prÃ©cision de ma recherche?
ExpÃ©rimentez avec diffÃ©rents modÃ¨les, ajustez la taille des chunks, et utilisez des techniques de query expansion.

---

## ğŸ‘¨â€ğŸ’» Auteurs

- ğŸš€ [Simon-Pierre Boucher](https://github.com/simonpierreboucher0) - CrÃ©ateur principal

---

<p align="center">
â­ N'oubliez pas de mettre une Ã©toile si ce projet vous a Ã©tÃ© utile! â­
</p>

<p align="center">
ğŸ”— <a href="https://github.com/simonpierreboucher0/embedding-api">GitHub</a> | 
ğŸ› <a href="https://github.com/simonpierreboucher0/embedding-api/issues">Signaler un problÃ¨me</a> | 
ğŸ’¡ <a href="https://github.com/simonpierreboucher0/embedding-api/discussions">Discussions</a>
</p>
