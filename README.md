# ğŸŒ Embedding API Gateway ğŸš€

## ğŸ”¥ Passerelle universelle pour tous vos services d'embeddings et recherche sÃ©mantique vectorielle ğŸ”¥

[![GitHub stars](https://img.shields.io/github/stars/simonpierreboucher0/embedding-api?style=social)](https://github.com/simonpierreboucher0/embedding-api/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.105.0+-green.svg)](https://fastapi.tiangolo.com/)

---

## âœ¨ CaractÃ©ristiques principales

ğŸ”„ **Interface unifiÃ©e** - Une API pour tous les fournisseurs d'embeddings  
ğŸ§© **Triple moteur de recherche vectorielle** - FAISS, ChromaDB ou LanceDB au choix  
ğŸ” **Recherche vectorielle avancÃ©e** - Recherche sÃ©mantique optimisÃ©e pour diffÃ©rents cas d'usage  
ğŸ“Š **Stockage flexible** - Formats optimisÃ©s pour rapiditÃ©, lisibilitÃ© ou Ã©volutivitÃ©  
âš¡ **Hautes performances** - Optimisations pour grands volumes de donnÃ©es  
ğŸ§  **Gestion d'index** - CrÃ©ation, mise Ã  jour et suppression simplifiÃ©es  
ğŸ”’ **SÃ©curitÃ© intÃ©grÃ©e** - Gestion sÃ©curisÃ©e des clÃ©s API  
ğŸŒˆ **Comparaison directe** - Calcul instantanÃ© de similaritÃ© entre textes  

---

## ğŸ¤– Fournisseurs et modÃ¨les pris en charge

### ğŸŸ¢ OpenAI
ModÃ¨les d'embeddings leaders du marchÃ©.

| ModÃ¨le | Dimension | Cas d'utilisation |
|--------|-----------|-------------------|
| ğŸŒŸ **text-embedding-ada-002** | 1536 | Recherche sÃ©mantique, classification, clustering |
| ğŸ’ **text-embedding-3-small** | 1536 | Applications avec contraintes de coÃ»t, performance Ã©quilibrÃ©e |
| ğŸ† **text-embedding-3-large** | 3072 | Recherche haute prÃ©cision, applications premium |

### ğŸŸ£ Cohere
ModÃ¨les d'embeddings spÃ©cialisÃ©s et multilingues.

| ModÃ¨le | Cas d'utilisation |
|--------|-------------------|
| ğŸŒ  **embed-english-v3.0** | Recherche sÃ©mantique optimisÃ©e pour l'anglais |
| ğŸŒ **embed-multilingual-v3.0** | Applications multilingues, recherche cross-language |
| âš¡ **embed-english-light-v3.0** | Version lÃ©gÃ¨re et rapide pour l'anglais |
| ğŸš€ **embed-multilingual-light-v3.0** | Version lÃ©gÃ¨re multilingue, faible latence |

### ğŸ”µ Mistral AI
ModÃ¨le d'embedding puissant conÃ§u en France.

| ModÃ¨le | Cas d'utilisation |
|--------|-------------------|
| ğŸ’« **mistral-embed** | Recherche sÃ©mantique, RAG, classification de documents |

### ğŸŸ¡ DeepInfra
Large sÃ©lection de modÃ¨les d'embeddings open-source optimisÃ©s.

| ModÃ¨le | Cas d'utilisation |
|--------|-------------------|
| ğŸ” **BAAI/bge-base-en-v1.5** | ModÃ¨le Ã©quilibrÃ© pour l'anglais |
| ğŸ” **BAAI/bge-small-en-v1.5** | Version compacte, faible empreinte mÃ©moire |
| ğŸ”¬ **BAAI/bge-large-en-v1.5** | PrÃ©cision Ã©levÃ©e pour applications critiques |
| ğŸŒŸ **thenlper/gte-large** | ModÃ¨le gÃ©nÃ©ral avec large couverture sÃ©mantique |
| â­ **thenlper/gte-base** | Version Ã©quilibrÃ©e pour usage gÃ©nÃ©ral |
| ğŸ“Š **intfloat/e5-large-v2** | OptimisÃ© pour RAG et recherche avancÃ©e |
| ğŸ§® **sentence-transformers/all-mpnet-base-v2** | Classique pour transformation de phrases |

### ğŸŸ  Voyage AI
ModÃ¨les d'embeddings spÃ©cialisÃ©s avec capacitÃ©s avancÃ©es.

| ModÃ¨le | Cas d'utilisation |
|--------|-------------------|
| ğŸŒŠ **voyage-large-2** | Recherche sÃ©mantique de pointe, applications premium |
| ğŸ§  **voyage-large-2-instruct** | Recherche sÃ©mantique guidÃ©e par instructions |
| ğŸ’» **voyage-code-2** | SpÃ©cialisÃ© pour code source et documentation technique |

---

## ğŸ”„ Bases de donnÃ©es vectorielles supportÃ©es

### ğŸŒŸ FAISS (Facebook AI Similarity Search)
BibliothÃ¨que de recherche vectorielle ultra-rapide.

| Points forts | Cas d'utilisation |
|--------------|-------------------|
| âš¡ **Ultra performant** | Grands volumes de donnÃ©es, recherche en temps rÃ©el |
| ğŸ“ **Recherche par distance euclidienne** | PrÃ©cision Ã©levÃ©e pour la recherche de similitude |
| ğŸš€ **OptimisÃ© pour le calcul distribuÃ©** | Applications Ã  l'Ã©chelle de production |

### ğŸ’  Chroma
Base de donnÃ©es vectorielle conÃ§ue pour les applications d'IA.

| Points forts | Cas d'utilisation |
|--------------|-------------------|
| ğŸ§© **API intuitive** | DÃ©veloppement rapide d'applications RAG |
| ğŸŒ **FlexibilitÃ© des mÃ©tadonnÃ©es** | Filtrage complexe, recherche hybride |
| ğŸ“Š **Collections et espaces de noms** | Organisation efficace des donnÃ©es |

### ğŸ“Š LanceDB
Base de donnÃ©es vectorielle orientÃ©e document avec persistance.

| Points forts | Cas d'utilisation |
|--------------|-------------------|
| ğŸ’¾ **Persistance intÃ©grÃ©e** | DonnÃ©es conservÃ©es entre les redÃ©marrages |
| ğŸ“ **Format Apache Arrow** | Performance et interopÃ©rabilitÃ© Ã©levÃ©es |
| ğŸ”„ **Mises Ã  jour performantes** | Index Ã©volutifs et modifiables |

### ğŸ“ SimilaritÃ© Cosinus
MÃ©thode simple de comparaison vectorielle (incluse pour compatibilitÃ©).

| Points forts | Cas d'utilisation |
|--------------|-------------------|
| ğŸ” **Transparence** | Stockage JSON lisible et facilement inspectable |
| ğŸ§ª **SimplicitÃ©** | Prototypage, petits projets, tests |
| ğŸ§® **PrÃ©cision de base** | Comparaisons directes sans optimisation spÃ©ciale |

---

## ğŸ› ï¸ Installation facile en 3 Ã©tapes

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/simonpierreboucher0/embedding-api.git
cd embedding-api
```

### 2ï¸âƒ£ Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configurer vos clÃ©s API
CrÃ©ez un fichier `.env` avec vos clÃ©s:
```ini
OPENAI_API_KEY=sk-xxxx
COHERE_API_KEY=xxxx
MISTRAL_API_KEY=xxxx
DEEPINFRA_API_KEY=xxxx
VOYAGE_API_KEY=xxxx
```

> ğŸ’¡ **Astuce**: Vous n'avez besoin de fournir que les clÃ©s pour les fournisseurs que vous allez utiliser!

---

## ğŸš€ Guide d'utilisation rapide

### â–¶ï¸ DÃ©marrer le serveur
```bash
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

### ğŸ“‹ Obtenir la liste des modÃ¨les et bases de donnÃ©es vectorielles
```bash
curl -X GET http://localhost:8000/models
curl -X GET http://localhost:8000/vector_dbs
```

### ğŸ§  GÃ©nÃ©rer des embeddings
```bash
curl -X POST http://localhost:8000/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-3-small",
    "texts": "Artificial intelligence is transforming the world."
  }'
```

### ğŸ—„ï¸ CrÃ©er un index avec FAISS
```bash
curl -X POST http://localhost:8000/index \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-3-small",
    "index_name": "ai_concepts",
    "texts": [
      "Artificial intelligence is the simulation of human intelligence processes by machines.",
      "Machine learning is a subset of AI that enables systems to learn from data.",
      "Deep learning is based on neural networks with many layers.",
      "Natural language processing allows machines to understand human language.",
      "Computer vision enables machines to interpret and make decisions based on visual input."
    ],
    "db_type": "faiss"
  }'
```

### ğŸŒŸ CrÃ©er un index avec ChromaDB
```bash
curl -X POST http://localhost:8000/index \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-3-small",
    "index_name": "ai_concepts_chroma",
    "texts": [
      "Artificial intelligence is the simulation of human intelligence processes by machines.",
      "Machine learning is a subset of AI that enables systems to learn from data.",
      "Deep learning is based on neural networks with many layers.",
      "Natural language processing allows machines to understand human language.",
      "Computer vision enables machines to interpret and make decisions based on visual input."
    ],
    "db_type": "chroma"
  }'
```

### ğŸ“Š CrÃ©er un index avec LanceDB
```bash
curl -X POST http://localhost:8000/index \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-3-small",
    "index_name": "ai_concepts_lance",
    "texts": [
      "Artificial intelligence is the simulation of human intelligence processes by machines.",
      "Machine learning is a subset of AI that enables systems to learn from data.",
      "Deep learning is based on neural networks with many layers.",
      "Natural language processing allows machines to understand human language.",
      "Computer vision enables machines to interpret and make decisions based on visual input."
    ],
    "db_type": "lancedb"
  }'
```

### ğŸ” Recherche sÃ©mantique
```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-3-small",
    "index_name": "ai_concepts",
    "query": "How do computers understand text?",
    "top_k": 3,
    "db_type": "faiss"
  }'
```

### ğŸ”„ Mettre Ã  jour un index existant
```bash
curl -X PUT http://localhost:8000/index/ai_concepts \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "text-embedding-3-small",
    "texts": [
      "Reinforcement learning is a training method based on rewarding desired behaviors.",
      "Transformer models have revolutionized natural language processing tasks."
    ],
    "db_type": "faiss"
  }'
```

### ğŸ§® Comparer deux textes directement
```bash
curl -X POST "http://localhost:8000/compare" \
  -d "provider=openai&model=text-embedding-3-small&texts=Artificial%20intelligence%20is%20transforming%20industries.&texts=AI%20is%20changing%20how%20businesses%20operate."
```

---

## ğŸ“Š Structure complÃ¨te des requÃªtes et rÃ©ponses

### ğŸ“ GÃ©nÃ©ration d'embeddings

**RequÃªte:**
```json
{
  "provider": "openai",              // ğŸŒ Fournisseur (obligatoire)
  "model": "text-embedding-3-small", // ğŸ¤– ModÃ¨le spÃ©cifique (obligatoire)
  "texts": ["Premier texte", "DeuxiÃ¨me texte"], // ğŸ“„ Textes (string ou array)
  "encoding_format": "float",        // ğŸ“Š Format d'encodage (pour certains providers)
  "input_type": "classification"     // ğŸ·ï¸ Type d'entrÃ©e (pour Cohere)
}
```

**RÃ©ponse:**
```json
{
  "embeddings": [                    // ğŸ§  Liste des vecteurs d'embedding
    [0.0023, -0.0118, 0.0094, ...],  // ğŸ“Š Premier vecteur
    [0.0089, -0.0342, 0.0211, ...]   // ğŸ“Š DeuxiÃ¨me vecteur
  ],
  "model": "text-embedding-3-small", // ğŸ¤– ModÃ¨le utilisÃ©
  "provider": "openai",              // ğŸŒ Fournisseur utilisÃ©
  "dimension": 1536,                 // ğŸ“ Dimension des vecteurs
  "total_tokens": 14                 // ğŸ”¢ Nombre de tokens (si disponible)
}
```

### ğŸ—„ï¸ CrÃ©ation d'index

**RequÃªte:**
```json
{
  "provider": "openai",              // ğŸŒ Fournisseur (obligatoire)
  "model": "text-embedding-3-small", // ğŸ¤– ModÃ¨le spÃ©cifique (obligatoire)
  "index_name": "mon_index",         // ğŸ“š Nom de l'index (obligatoire)
  "texts": [                         // ğŸ“„ Textes Ã  indexer (obligatoire)
    "Premier document Ã  indexer",
    "DeuxiÃ¨me document Ã  indexer",
    "TroisiÃ¨me document Ã  indexer"
  ],
  "db_type": "faiss",                // ğŸ’¾ Type de base de donnÃ©es: "faiss", "chroma", "lancedb" ou "cosine"
  "encoding_format": "float",        // ğŸ“Š Format d'encodage (optionnel)
  "input_type": "classification"     // ğŸ·ï¸ Type d'entrÃ©e (pour Cohere, optionnel)
}
```

**RÃ©ponse:**
```json
{
  "provider": "openai",              // ğŸŒ Fournisseur utilisÃ©
  "model": "text-embedding-3-small", // ğŸ¤– ModÃ¨le utilisÃ©
  "index_name": "mon_index",         // ğŸ“š Nom de l'index crÃ©Ã©
  "total_chunks": 3,                 // ğŸ§© Nombre de documents indexÃ©s
  "created_at": "2023-08-15T14:23:45.123456", // â° Date de crÃ©ation
  "dimension": 1536,                 // ğŸ“ Dimension des vecteurs
  "db_type": "faiss"                 // ğŸ’¾ Type de base de donnÃ©es utilisÃ©e
}
```

### ğŸ” Recherche sÃ©mantique

**RequÃªte:**
```json
{
  "provider": "openai",              // ğŸŒ Fournisseur (obligatoire)
  "model": "text-embedding-3-small", // ğŸ¤– ModÃ¨le spÃ©cifique (obligatoire)
  "index_name": "mon_index",         // ğŸ“š Nom de l'index (obligatoire)
  "query": "Ma requÃªte de recherche", // ğŸ” Texte de la requÃªte (obligatoire)
  "top_k": 5,                        // ğŸ” Nombre de rÃ©sultats souhaitÃ©s
  "db_type": "faiss"                 // ğŸ’¾ Type de base de donnÃ©es: "faiss", "chroma", "lancedb" ou "cosine"
}
```

**RÃ©ponse:**
```json
{
  "index_name": "mon_index",         // ğŸ“š Nom de l'index utilisÃ©
  "provider": "openai",              // ğŸŒ Fournisseur utilisÃ©
  "model": "text-embedding-3-small", // ğŸ¤– ModÃ¨le utilisÃ©
  "query": "Ma requÃªte de recherche", // ğŸ” Texte de la requÃªte
  "db_type": "faiss",                // ğŸ’¾ Type de base de donnÃ©es utilisÃ©e
  "results": [                       // ğŸ“‹ RÃ©sultats de recherche
    {
      "chunk_id": 2,                 // ğŸ†” ID du document
      "text": "DeuxiÃ¨me document Ã  indexer", // ğŸ“„ Texte du document
      "distance": 0.125,             // ğŸ“ Distance (varie selon la base de donnÃ©es)
      "score": 0.89,                 // ğŸ“Š Score de similaritÃ©
      "rank": 1                      // ğŸ… Rang dans les rÃ©sultats
    },
    // ... autres rÃ©sultats
  ]
}
```

---

## ğŸ§ª Comparaison des bases de donnÃ©es vectorielles

### ğŸš€ FAISS (Facebook AI Similarity Search)

**Avantages:**
- âš¡ **Ultra rapide** pour les grands ensembles de donnÃ©es
- ğŸ” **Recherche approximative** optimisÃ©e pour hautes dimensions
- ğŸ“ˆ **Passage Ã  l'Ã©chelle** efficace avec des millions de vecteurs
- ğŸ§  **Optimisations mÃ©moire** pour les grandes collections

**Utilisez FAISS quand:**
- ğŸ—„ï¸ Vous avez de grandes collections de documents (>10K textes)
- â±ï¸ La vitesse de recherche est critique
- ğŸ’½ Vous avez des contraintes de mÃ©moire pour de grands index

### ğŸ’  ChromaDB

**Avantages:**
- ğŸ”„ **IntÃ©gration native** avec des systÃ¨mes RAG
- ğŸ§  **ConÃ§u pour l'IA** et optimisÃ© pour les embeddings
- ğŸ” **Recherche hybride** combinant vecteurs et mÃ©tadonnÃ©es
- ğŸ“¦ **Collections organisÃ©es** pour structurer vos donnÃ©es

**Utilisez ChromaDB quand:**
- ğŸ¤– Vous dÃ©veloppez des applications RAG complexes
- ğŸ“ Vous avez besoin de filtres et mÃ©tadonnÃ©es Ã©laborÃ©s
- ğŸ“š Vous gÃ©rez plusieurs collections liÃ©es
- ğŸ”„ Vous voulez une solution adaptÃ©e aux workflows d'IA

### ğŸ“Š LanceDB

**Avantages:**
- ğŸ’½ **Persistance native** des donnÃ©es entre redÃ©marrages
- ğŸ“ **Format Apache Arrow** pour performance optimale
- ğŸ”„ **RequÃªtes complexes** sur donnÃ©es vectorielles
- ğŸ§© **OrientÃ© document** pour donnÃ©es structurÃ©es

**Utilisez LanceDB quand:**
- ğŸ’¾ La persistance des donnÃ©es est essentielle
- ğŸ“š Vous manipulez des documents avec structure complexe
- ğŸ”„ Vous avez besoin de mises Ã  jour frÃ©quentes de l'index
- ğŸ“ˆ Vous cherchez un compromis entre performance et fonctionnalitÃ©s

### ğŸ“ SimilaritÃ© Cosinus (avec stockage JSON)

**Avantages:**
- ğŸ“‹ **Simple et transparent** - stockage en fichiers JSON lisibles
- ğŸ¯ **PrÃ©cision Ã©levÃ©e** pour les petites collections
- ğŸ”§ **Facile Ã  dÃ©boguer** et Ã  inspecter manuellement
- ğŸ”„ **Portable** entre diffÃ©rents environnements

**Utilisez Cosinus quand:**
- ğŸ“ Vous avez de petites collections (< 10K textes)
- ğŸ‘ï¸ La lisibilitÃ©/transparence des donnÃ©es est importante
- ğŸ§ª Vous prototypez ou testez votre application

---

## ğŸ’¡ Cas d'utilisation avancÃ©s

### ğŸ”„ ChaÃ®ne de traitement RAG (Retrieval Augmented Generation)

```python
import requests
import json

# 1. Indexer des documents techniques
documents = [
    "FastAPI est un framework web moderne pour Python.",
    "FAISS est une bibliothÃ¨que pour la recherche de similaritÃ© efficace.",
    "Les embeddings vectoriels reprÃ©sentent le texte dans un espace multidimensionnel.",
    "La recherche sÃ©mantique permet de trouver des informations par leur signification.",
    "La distance cosinus est une mesure courante de similaritÃ© entre vecteurs."
]

# CrÃ©er l'index avec ChromaDB pour une utilisation optimale dans les flux RAG
requests.post(
    "http://localhost:8000/index",
    json={
        "provider": "openai",
        "model": "text-embedding-3-small",
        "index_name": "docs_techniques",
        "texts": documents,
        "db_type": "chroma"
    }
)

# 2. Quand l'utilisateur pose une question
user_query = "Comment fonctionne la recherche vectorielle?"

# 3. Rechercher les documents pertinents
search_response = requests.post(
    "http://localhost:8000/search",
    json={
        "provider": "openai",
        "model": "text-embedding-3-small",
        "index_name": "docs_techniques",
        "query": user_query,
        "top_k": 2,
        "db_type": "chroma"
    }
)

relevant_docs = [result["text"] for result in search_response.json()["results"]]

# 4. Construire un prompt enrichi pour un LLM
context = "\n".join(relevant_docs)
prompt = f"""
Contexte:
{context}

Question: {user_query}

RÃ©ponse basÃ©e sur le contexte donnÃ©:
"""

# 5. Envoyer Ã  un LLM (via un service externe comme OpenAI)
# Code pour appeler un LLM avec le prompt enrichi
print("Prompt enrichi avec contexte:", prompt)
```

### ğŸ§ª Test A/B de diffÃ©rentes bases de donnÃ©es vectorielles

```python
import requests
import time
import numpy as np
from tabulate import tabulate

# Bases de donnÃ©es vectorielles Ã  comparer
db_types = ["faiss", "chroma", "lancedb", "cosine"]

# Documents de test
documents = [
    "Artificial intelligence is the simulation of human intelligence in machines.",
    "Machine learning algorithms improve automatically through experience.",
    "Neural networks are computing systems inspired by biological neural networks.",
    "Deep learning is part of a broader family of machine learning methods.",
    "Natural language processing helps computers understand human language.",
    "Computer vision enables machines to interpret visual information.",
    "Reinforcement learning is learning what to do to maximize a reward signal.",
    "Supervised learning uses labeled training data to learn the mapping function.",
    "Unsupervised learning finds patterns in data without pre-existing labels.",
    "Transfer learning reuses a pre-trained model on a new problem."
]

# CrÃ©ation des index
for db_type in db_types:
    requests.post(
        "http://localhost:8000/index",
        json={
            "provider": "openai",
            "model": "text-embedding-3-small",
            "index_name": f"test_{db_type}",
            "texts": documents,
            "db_type": db_type
        }
    )
    print(f"Index crÃ©Ã© avec {db_type}")

# RequÃªtes de test
test_queries = [
    "How do computers learn from data?",
    "What is the relationship between AI and neural networks?",
    "How do machines understand images?",
    "What are different learning approaches in AI?"
]

# Variables pour collecter les rÃ©sultats
results = {db_type: {"latency": [], "first_result": []} for db_type in db_types}

# Effectuer les tests
for query in test_queries:
    print(f"\nTest de la requÃªte: '{query}'")
    
    for db_type in db_types:
        # Mesurer le temps de rÃ©ponse
        start_time = time.time()
        response = requests.post(
            "http://localhost:8000/search",
            json={
                "provider": "openai",
                "model": "text-embedding-3-small",
                "index_name": f"test_{db_type}",
                "query": query,
                "top_k": 3,
                "db_type": db_type
            }
        )
        latency = time.time() - start_time
        
        # Collecter les rÃ©sultats
        search_results = response.json()["results"]
        first_result = search_results[0]["text"] if search_results else "Aucun rÃ©sultat"
        
        # Stocker les mesures
        results[db_type]["latency"].append(latency)
        results[db_type]["first_result"].append(first_result)
        
        print(f"  {db_type}: {latency:.4f}s - Premier rÃ©sultat: '{first_result[:50]}...'")

# Analyser les rÃ©sultats
performance_data = []
for db_type in db_types:
    avg_latency = np.mean(results[db_type]["latency"])
    performance_data.append([
        db_type,
        f"{avg_latency:.4f}s",
        f"{min(results[db_type]['latency']):.4f}s",
        f"{max(results[db_type]['latency']):.4f}s"
    ])

# Afficher les rÃ©sultats dans un tableau
print("\n=== RÃ‰SULTATS DE PERFORMANCE ===")
print(tabulate(
    performance_data,
    headers=["Base de donnÃ©es", "Latence moyenne", "Latence min", "Latence max"],
    tablefmt="grid"
))

# Nettoyage - supprimer les index de test
for db_type in db_types:
    requests.delete(f"http://localhost:8000/index/test_{db_type}?db_type={db_type}")
    print(f"Index test_{db_type} supprimÃ©")
```

---

## ğŸ“š Guide des fonctionnalitÃ©s avancÃ©es

### ğŸ§® Comparaison directe de textes

Calculez rapidement la similaritÃ© entre deux textes sans crÃ©er d'index:

```bash
curl -X POST "http://localhost:8000/compare?texts=Artificial%20intelligence%20is%20revolutionizing%20industries.&texts=AI%20is%20changing%20how%20businesses%20operate.&provider=openai&model=text-embedding-3-small"
```

RÃ©ponse:
```json
{
  "text1": "Artificial intelligence is revolutionizing industries.",
  "text2": "AI is changing how businesses operate.",
  "similarity": 0.8712,
  "provider": "openai",
  "model": "text-embedding-3-small"
}
```

### ğŸ“‹ Listing des index disponibles

```bash
curl -X GET http://localhost:8000/indexes
```

RÃ©ponse:
```json
[
  {
    "provider": "openai",
    "model": "text-embedding-3-small",
    "index_name": "ai_concepts",
    "total_chunks": 8,
    "created_at": "2023-08-15T14:23:45.123456",
    "updated_at": "2023-08-16T09:12:34.567890",
    "dimension": 1536,
    "db_type": "faiss"
  },
  {
    "provider": "openai",
    "model": "text-embedding-3-small",
    "index_name": "ai_concepts_chroma",
    "total_chunks": 5,
    "created_at": "2023-08-15T15:45:12.345678",
    "dimension": 1536,
    "db_type": "chroma"
  },
  {
    "provider": "cohere",
    "model": "embed-english-v3.0",
    "index_name": "ai_concepts_lance",
    "total_chunks": 5,
    "created_at": "2023-08-15T16:30:22.345678",
    "dimension": 1024,
    "db_type": "lancedb"
  }
]
```

### ğŸ” Obtenir les informations sur un index spÃ©cifique

```bash
curl -X GET "http://localhost:8000/index/ai_concepts?db_type=faiss"
```

RÃ©ponse:
```json
{
  "provider": "openai",
  "model": "text-embedding-3-small",
  "index_name": "ai_concepts",
  "total_chunks": 8,
  "created_at": "2023-08-15T14:23:45.123456",
  "updated_at": "2023-08-16T09:12:34.567890",
  "dimension": 1536,
  "db_type": "faiss"
}
```

---

## ğŸ”’ SÃ©curitÃ© et bonnes pratiques

- ğŸ”‘ **Gestion des clÃ©s**: StockÃ©es localement dans `.env`, jamais exposÃ©es
- ğŸ›¡ï¸ **CORS**: Configuration sÃ©curisÃ©e pour les requÃªtes cross-origin
- ğŸ“ **Logging**: DÃ©tails utiles sans informations sensibles
- ğŸ”„ **Rate Limiting**: Respect des limites de taux des fournisseurs
- ğŸ’¾ **Stockage local**: Les donnÃ©es restent sur votre serveur, pas d'envoi externe
- ğŸ§¹ **Nettoyage facile**: Suppression d'index via API simple

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues! Voici comment participer:

1. ğŸ´ **Fork** le dÃ©pÃ´t
2. ğŸ”„ CrÃ©ez une **branche** (`git checkout -b feature/ma-fonctionnalite`)
3. âœï¸ Faites vos **modifications**
4. ğŸ“¦ **Commit** vos changements (`git commit -m 'Ajout de ma fonctionnalitÃ©'`)
5. ğŸ“¤ **Push** vers la branche (`git push origin feature/ma-fonctionnalite`)
6. ğŸ” Ouvrez une **Pull Request**

### ğŸ’¼ IdÃ©es de contributions

- ğŸ§ª Support de nouveaux fournisseurs d'embeddings
- ğŸ’¾ IntÃ©gration d'autres bases de donnÃ©es vectorielles (PGVector, Qdrant, Weaviate...)
- ğŸ“ AmÃ©lioration de la documentation
- âœ¨ FonctionnalitÃ©s avancÃ©es (clustering, chunking automatique)
- ğŸš€ Optimisations de performance
- ğŸŒ Support pour la multimodalitÃ© (embeddings d'images, audio, etc.)

---

## ğŸ“œ Licence

Ce projet est sous licence [MIT](LICENSE) - voir le fichier LICENSE pour plus de dÃ©tails.

---

## â“ FAQ

### ğŸ”„ Quelle base de donnÃ©es vectorielle choisir?
- **FAISS** pour performance pure et grands volumes de donnÃ©es
- **ChromaDB** pour applications RAG et intÃ©gration IA avancÃ©e
- **LanceDB** pour persistance, requÃªtes complexes et donnÃ©es structurÃ©es
- **Cosine** pour prototypage et petits ensembles de donnÃ©es

### ğŸ” Quelle est la meilleure dimension pour les embeddings?
En gÃ©nÃ©ral, plus la dimension est Ã©levÃ©e, plus la prÃ©cision est grande, mais au prix de plus de ressources. 1536 est un bon compromis, 3072 pour une prÃ©cision maximale.

### ğŸ§© Comment chunker mes documents avant de les indexer?
Utilisez une bibliothÃ¨que comme LangChain ou LlamaIndex pour dÃ©couper vos documents avant de les envoyer Ã  l'API.

### ğŸ’° Comment optimiser les coÃ»ts des API d'embeddings?
GÃ©nÃ©rez les embeddings une seule fois et stockez-les. Utilisez des modÃ¨les plus lÃ©gers pour les cas d'usage moins critiques.

### ğŸ”§ Comment puis-je amÃ©liorer la prÃ©cision de ma recherche?
ExpÃ©rimentez avec diffÃ©rents modÃ¨les, ajustez la taille des chunks, et utilisez des techniques de query expansion.

### ğŸ“¦ Comment installer les bases de donnÃ©es optionnelles?
Installez ChromaDB avec `pip install chromadb` et LanceDB avec `pip install lancedb`.

---

## ğŸ‘¨â€ğŸ’» Auteurs

- ğŸš€ [Simon-Pierre Boucher](https://github.com/simonpierreboucher0) - CrÃ©ateur principal

---

<p align="center">
â­ N'oubliez pas de mettre une Ã©toile si ce projet vous a Ã©tÃ© utile! â­
</p>

<p align="center">
ğŸ”— <a href="https://github.com/simonpierreboucher0/embedding-api">GitHub</a> | 
ğŸ› <a href="https://github.com/simonpierreboucher0/embedding-api/issues">Signaler un problÃ¨me</a> | 
ğŸ’¡ <a href="https://github.com/simonpierreboucher0/embedding-api/discussions">Discussions</a>
</p>
